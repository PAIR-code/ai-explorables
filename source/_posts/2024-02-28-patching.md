---
template: post_v2.html
title: Can Large Language Models Explain Their Internal Mechanisms?
socialsummary: An interactive introduction to Patchscopes, an inspection framework for explaining the hidden representations of large language models, with large language models. 
shareimg: 
shareimgabstract: 
authors: Nada Hussein<a class='footstart small' key=author></a>, Asma Ghandeharioun<a class='footstart small' key=author></a>, Ryan Mullins, Emily Reif, Jimbo Wilson, Nithum Thain<a class='footstart small' key=author2></a>, Lucas Dixon<a class='footstart small' key=author2></a>
date: July 2024
permalink: /patchscopes/
---

The “mind” of a Large Language Model (LLM) consists of layers of interconnected artificial neurons. These layers communicate with vectors of numbers, often called the *hidden representations*. Deriving human-comprehensible meaning from such internals of AI systems is the focus of research into *machine-learning interpretability,* which has been making many exciting advances recently<a class='footstart' key='sae'></a>.

This Explorable is about one of these advances: a new family of interpretability methods called Patchscopes<a class='citestart' key='ghandeharioun'></a>. The idea is to perform a kind of surgery on the neurons of an LLM, cutting out and replacing hidden representations between different prompts and layers. The key concept is the *inspection prompt,* which acts as a lens into the mind of an LLM, allowing the model itself to help uncover human-interpretable meaning. 

<div style="margin: 0 auto; width: fit-content;">
  <img src="https://storage.googleapis.com/uncertainty-over-space/explorables/patching/intro_image.svg" />
</div>

Patchscopes is built on an understanding of LLMs and the transformer architecture. For a deeper dive into transformers and the schematic diagrams we use throughout this Explorable, see [Appendix A](#appendix-a-a-brief-review-of-transformers).

### Letting representations “talk” with Patchscopes
The Patchscopes framework leverages a simple premise: LLMs have the inherent ability to translate their own seemingly inscrutable hidden representations into human understandable text. Patching hidden representations between locations during inference allows us to inspect the information within a hidden representation, understand LLM behavior, or even augment the LLM's behavior to improve its performance. Let’s explore this concretely with a step-by-step example.

<script type="application/x-vis+json">
{
  "type": "patching-walkthrough",
  "jsonDataUrl": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/uk_example.json"
}
</script>

Patchscopes is a surprisingly versatile tool. It can be used for tasks like extracting factual information, such as the country in which Barcelona is located; understanding the mechanisms of model refusal<a class='citestart' key='xstest'></a>, such as refusing to respond to a prompt that is *perceived* to come from a person with malicious intent<a class='citestart' key='whos-asking'></a>; debugging incorrect outputs; or even finding latent harmful information that might exist in a model even if it isn’t verbalized<a class='citestart' key='selfie whos-asking'></a>.

In the following sections we explore three case studies in which Patchscopes is used to: (1) [discover](#case-study-verbalizing-the-model-s-thought-process) how models resolve entities in their early layers; (2) [evaluate](#case-study-extracting-latent-attributes) how accurately the model’s hidden representation captures well-known concepts; and (3) [augment](#application-correcting-reasoning-errors) the model’s processing of complex questions. These case studies touch upon some of the aspects of Patchscopes that require human judgment,  such as determining good target locations for a given source location, or how to construct prompts that enable model inspection—although there are [many more](#discussion-and-open-questions) dimensions and design considerations to be explored. (For a formal definition of Patchscopes, see [Appendix B](#appendix-b-formal-description-of-patchscopes).)

Patchscopes is being actively developed, and we invite your explorations, commentary, and feedback [on GitHub](https://github.com/PAIR-code/interpretability/issues).
### Case-study: Verbalizing the model’s “thought” process
An LLM’s ability to perform a task—answering questions, summarizing documents, translating languages—is dependent on its ability to correctly contextualize the tokens in its prompt. For example, to answer the question "When was Diana, Princess of Wales born?" the model must understand that "Diana" refers to "Princess Diana", rather than the generic name, or the ancient Roman goddess of the hunt. At what point does the model make this association, if at all?

In this case study, we use Patchscopes to shed light on how models process tokens across the layers of the model, and update their hidden representations to resolve what an entity refers to. We focus on understanding hidden representations of ***named entities***—people, places, movies, etc.

#### Entity resolution & establishing context from a prompt

We are first going to identify how the model resolves the entity <span class=token>Diana</span> from a source prompt. This is the first step in understanding how the model stores and accesses factual information, which can enable other tasks such as correcting outdated information. Building on the Princess Diana example, we construct a simple source prompt using her name and title:

<script type="application/x-vis+json">
{
   "type": "tokens",
  "tokens": ["Diana, Princess of Wales"]
}
</script>

We can measure how an entity is resolved by having the model generate a description of the entity and compare it to a known factual description. We can trigger this generation with a [few-shot](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/few-shot-examples) inspection prompt<a class='citestart' key='brown'></a> consisting of entity names followed by their description, and a final placeholder token <span class="token highlight target">x</span>, which will be used as the target patching location.

<script type="application/x-vis+json">
{
   "type": "tokens",
  "tokens": ["Syria: Country in the Middle East,", "Leonardo DiCaprio: American actor,", "Samsung: South Korean multinational major appliance and consumer electronics corporation,", "x"],
   "highlightLast": "true",
   "target": "true",
   "newLine": "true"
}
</script>

We feed the source prompt into a 40 layer model and extract the hidden representation of the <span class=token>Wales</span> token at every layer. Next, each hidden representation is patched into the first layer in place of the <span class='token highlight target'>x</span> token in the inspection prompt. Finally, the LLM continues decoding on this patched inspection prompt to generate the entity description for the given hidden representation of the <span class=token>Wales</span> token.

In the table below, we show the generated descriptions from the first 10 source layers, and use an LLM-based automated evaluator<a class='citestart' key='autoeval'></a> to score the similarity of each output to the Wikipedia description of the original source prompt on a scale of 0-10<a class='footstart' key='prompt'></a>.

<script type="application/x-vis+json">
{
   "type": "entity-description",
  "jsonDataUrl": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/entity_postprocessed/Diana,_Princess_of_Wales_flat_data.json",
"tooltips": {"0": "In layers 0-2, the hidden representation reflects the subject 'Wales'", "3": "In layers 3-4, the hidden representation reflects the subject 'Princess of Wales'", "7":"Finally at layer 7, the model resolves the entire entity, 'Diana, Princess of Wales'"}
}
</script>

Looking across all 40 layers, we start to see a pattern in the per-layer automated evaluator scores – that entity resolution typically happens in the early layers (< L=20) of the model. The general pattern of resolving in the early layers corresponds with theories about layer function<a class='citestart' key='bert-rediscovers2019 bottomup2019'></a>: the role of ***early layers is to establish context from the prompt***.

<script type="application/x-vis+json">
{
   "type": "entity-description-scores",
"jsonDataUrl": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/entity_postprocessed/Diana,_Princess_of_Wales_flat_data.json"
}
</script>

#### Tokenization can change how each layer processes information
Tokenization<a class='citestart' key='tokenization'></a> differs between model families, and has a strong impact on how the model navigates its embedding space. In the following visualization, we’ll focus on a tokenized representation of an example source prompt, "Dubai":

<script type="application/x-vis+json">
{
   "type": "tokens",
  "tokens": ["Dub","ai" ]
}
</script>

Using the same few-shot entity description inspection prompt seen above, this example demonstrates the influence of token-by-layer processing on the generated text – we see the outputs change as the model incorporates more tokens into its context.

<script type="application/x-vis+json">
{
   "type": "entity-description",
  "jsonDataUrl": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/entity_postprocessed/Dubai_flat_data.json",
"tooltips": {"0": "In layer 0, the entity description reflects the subject 'AI', due to the tokenization of Dubai", "1": "From layer 1 onward, the full source prompt 'Dubai' is processed"}
}
</script>

#### From TV shows to countries: Exploring classes of named entities
The visualization below allows you to explore more examples<a class='footstart' key='dataset'></a> of named entities on your own. While this dataset is by no means exhaustively descriptive of the model’s behavior, we do see differences in where the model resolves the target entity across different classes—TV shows, people, countries—and even across token counts. Careful observers will notice a pattern in the way the model processes information: it builds up context by ***sequentially processing tokens in reverse order***, gradually incorporating each token into the hidden representation.  Further work is required to understand these patterns at scale<a class='footstart' key='incorrect'></a>.

<script type="application/x-vis+json">
{
   "type": "entity-description-interactive",
  "jsonDataUrl": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/entity/Diana,_Princess_of_Wales_flat_data.json"
}
</script>

To quickly recap this case study, we see that Patchscopes provides a *highly flexible* method for defining experiments that extract, verify, and characterize the model’s information retrieval process. It demonstrates *highly expressive generations* in early layers, where models are believed to do this processing.
### Case-study: Extracting latent attributes
We can also use Patchscopes to take this a step further, exploring how a model relates subjects to their specific attributes at various model layers.
#### Feature extraction to corroborate hidden representations
Consider the subject <span class=token>Spain</span>. Can we determine whether our LLM is able to gather context about Spain and correctly identify attributes like its largest city or official currency? More generally, given a hidden representation of a subject, we can explore whether an LLM can extract a specific attribute of that subject.

With Patchscopes, we can formulate an inspection prompt to extract an attribute. This prompt consists of a description of the attribute and a placeholder token, <span class='token highlight target'>x</span>, into which the hidden representation of the source subject is patched. For example, if we’re interested in the feature "the largest city" of a certain country, an inspection prompt that should generate the desired attribute in text looks like:

<script type="application/x-vis+json">
{
  "type": "tokens",
  "tokens": ["The largest city in", "x"],
  "highlightLast": "true",
  "target": "true"
}
</script>

To extract the feature, we first capture the hidden representation from the last token of the source prompt and patch it into the <span class='token highlight target'>x</span> token in the inspection prompt above. We then determine if the correct answer, in this case <span class=token>Madrid</span>, appears in the target model’s output. The visualization below lets you explore this example for a range of source layers.

<script type="application/x-vis+json">
{
  "type": "probing-vs-patching-intro",
  "dataDir": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/probing_vs_patching"
}
</script>

#### Comparing Patchscopes to probing
The most comparable approach to this problem is a technique called *probing* – training a classifier (such as logistic regression) on a subset of data to allow it to predict an attribute from a hidden representation. This approach has a few downsides – namely, it requires a pre-specified subset of labels, which can limit expressivity of the output, and requires dedicated data to train the classifier.

Unlike probing, Patchscopes does not require any labeled data or supervised training. Additionally, we show that Patchscopes is more accurate than probing in many contexts<a class='footstart' key='dataset'></a> in the full paper<a class='citestart' key='ghandeharioun'></a>. The visualization below lets us explore a variety of reasoning tasks and compare the performance of patching and probing<a class='footstart' key='probing-viz'></a>.

<script type="application/x-vis+json">
{
  "type": "probing-vs-patching",
  "dataDir": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/probing_vs_patching"
}
</script>

A few noticeable patterns emerge from these examples. ***Patchscopes outperforms probing in early layers*** on 7 of the 8 tasks. However, in 4 of the 8 tasks, probing outperforms patching in some later layers<a class='footstart' key='probing-performance'></a>.

Importantly, these results suggest that ***Patchscopes is a viable alternative to probing classifiers***, while offering two improvements. First, the flexibility and simplicity of construction for feature extraction inspection prompts indicate that the prompts could be easily generated from existing, trusted structured data that enables direct performance comparison, such as [Wikipedia’s RDF dumps](https://www.wikidata.org/wiki/Wikidata:Database_download#RDF_dumps). Second, it does not require any additional training data or a predefined set of labels.
### Application: Correcting reasoning errors
In the case studies above, we demonstrate that Patchscopes is a flexible and expressive tool for *inspecting* a model. Can we go further and use the same framework to *change* model behavior to improve its performance?

Consider multi-hop reasoning, a problem formulation where the answer depends on making logical connections between disjointed pieces of information. For example, to determine the answer to ‘the largest city in sushi’s country of origin’, a model needs to correctly recognize that Sushi’s country of origin is Japan in order to answer with Japan’s largest city, Tokyo. 

One possible performance inhibitor on these problems may be the model failing to conduct sequential reasoning in the right order. Chain-of-thought (CoT) style prompts <a class='citestart' key='weiCoT'></a> improve performance on multi-hop reasoning problems<a class='citestart' key='llama'></a> by explicitly expressing reasoning as a series of steps during generation, but there is still room for improvement. Patchscopes may provide an alternative mechanism to control the order of reasoning steps and ultimately correct the generated output<a class='footstart' key='cot'></a>.
#### Patching backwards
To explore this, we constructed a small dataset<a class='footstart' key='dataset'></a> of two-clause multi-hop reasoning queries. We used examples from the [attribute extraction case study](#case-study-extracting-latent-attributes), where the first clause defines the scope of the answer, and the second clause establishes the necessary context. The model can correctly answer each of these clauses independently, but fails to answer the composite multi-hop query. With some knowledge about the query structure, we can create a Patchscope that works backward, patching to an earlier token in the same prompt<a class='footstart' key='backwardpatch'></a> to intervene and correct its answer, as shown in the example below:

<script type="application/x-vis+json">
{
  "type": "multihop-single",
  "jsonDataUrl": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/multihop/flat_multihop_data.json"
}
</script>

By generalizing the query structure that defines the Patchscope above, we start to see some patterns across our multi-hop reasoning queries:

<script type="application/x-vis+json">
{
  "type": "multihop-diff",
  "jsonDataUrl": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/multihop/flat_multihop_data_3.json"
}
</script>

#### Explore the accuracy of patched generations
The visualization below allows you to explore these examples individually. Use the accuracy grid in the center to explore the <span class="underline-patching">patched generations</span> for different source and target layer pairs, and compare them to the model’s <span class="underline-baseline">baseline response</span>. As described above, the hidden representation is extracted from the <span class="underline-entity">last token</span> in the multi-hop query, and we show the associated <span class="underline-entity">entity description</span><a class='footstart' key='widget-prompt'></a> below the accuracy grid to help you locate useful source layers.

<script type="application/x-vis+json">
{
  "type": "multihop",
  "jsonDataUrl": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/multihop/flat_multihop_data_3.json"
}
</script>

The findings presented above are by no means conclusive, but they do demonstrate the utility of Patchscopes as a method for making sense of model behavior regardless of whether that model is generating the “correct” output.

### Discussion and open questions
In the explorations above, we introduced Patchscopes—a method for understanding hidden representations in models—and established it as a flexible, highly expressive tool [for](#case-study-verbalizing-the-model-s-thought-process) [understanding](#case-study-extracting-latent-attributes) and even [augmenting](#application-correcting-reasoning-errors) model behavior, which improves and unifies prior work (e.g., vocabulary projection<a class='citestart' key='belrose lesswrong'></a>, probing classifiers<a class='citestart' key='alain belinkov'></a>, and computational interventions<a class='citestart' key='meng'></a>) into a shared theoretical framework. We are excited that the core idea of Patchscopes has already been adopted more broadly<a class='citestart' key='selfie'></a> in interpretability research as a stepping stone to advance our work on model understanding.

The case studies explored in this post capture a range of scenarios from input processing to attribute extraction to factual reasoning correction, but LLMs are used for an incredibly wide array of tasks, including mathematical reasoning, classification, code completion, and more. Each of these tasks provide many prompts to analyze, but significant research is required to understand how to create the relevant source or inspection prompts that enable human inspection of the model’s reasoning processes in these tasks. While we do not yet have generalizable heuristics or design guidelines, we do expect that certain classes of inspection prompts will have wide applicability as intermediate inspection tools across tasks that support the creation of task-specific inspection prompts. Examples of these include, but are not limited to, the [few-shot entity description](#case-study-verbalizing-the-model-s-thought-process) and [zero-shot attribute extraction](#case-study-extracting-latent-attributes) prompts described in this post.

Our overarching objective is to develop guidelines and [tools](https://github.com/PAIR-code/interpretability/tree/master/patchscopes/) that enable the effective application of Patchscopes across the widest possible variety of modeling tasks. To this end, we are exploring several research directions, such as broadening task applications, automating task-specific and task-agnostic patching configurations, and exploring non-identity hidden representation transforms.

The effectiveness of a patching configuration—the layer, prompt, and token position choices—depends on how the information propagates during inference. In the experiments we discussed above, simple heuristics worked well in configuring an effective Patchscope. When using the same source and target model, picking the same layer for source and target is a good start. If the analytical goal is more expressive, open-ended generation, e.g., to enable verification, we may pick target layers earlier than the source layer, as we did for [entity resolution](#case-study-verbalizing-the-model-s-thought-process). Patching into later target layers seems to be less useful generally, as shown in the [multi-hop reasoning](#application-correcting-reasoning-errors), likely because the model has already shifted from sense-making to decoding. Considering token positions, a rule of thumb is that picking late target positions minimizes the chances of placeholder contamination<a class='footstart' key='placeholder-contamination'></a>, and therefore is more likely to work than other alternatives. If we aim to ask more complex questions, and therefore use more complicated inspection prompts, prior knowledge about the model's information flow<a class='footstart' key='circuits'></a> may be required, or we might need to test many configuration options before finding one that is most effective. Coming up with effective Patchscopes configurations automatically would make this framework much more powerful, and is a research direction we are currently exploring.

Finally, a full Patchscopes configuration allows for the application of transforms to the hidden representation between the extraction and injection steps. The examples in this explorable only employ the identity function as the transform, but the paper<a class='citestart' key='ghandeharioun'></a> shows that, surprisingly, it is indeed possible to use a larger and more expressive model from the same family to explain a smaller model using an [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation) function.

But what about different hidden representation dimensions, models from different families, or even models trained on different modalities of data? With more complex transformations, how far can we push the limits in cross-model Patchscopes? Answering these questions requires more research, and we continue to explore these ideas.

We encourage you to check out our paper<a class='citestart' key='ghandeharioun'></a> or reach out to us [on GitHub](https://github.com/PAIR-code/interpretability/issues) for more information about Patchscopes.
### Credits
Thanks to Avi Caciularu, Carter Blum, Mor Geva, Mahima Pushkarna, Ian Tenney, Fernanda Viegas, Martin Wattenberg, David Weinberger, and James Wexler for their help with this piece.

### Appendix A: A brief review of transformers
LLMs are transformer models that take a text string as input, called a *prompt*, that describes a task, such as answering a question, summarizing an article, translating text, generating code snippets. The LLM’s job is to generate the text that best follows from the task captured in the prompt. For an illustrative example, consider the prompt: "United Kingdom".

This input text is tokenized<a class='footstart' key='tokens'></a> – broken up into the model’s atomic vocabulary consisting of individual words and small sequences of characters<a class='citestart' key='tokenization'></a>.

<script type="application/x-vis+json">
{
  "type": "tokens",
  "tokens": ["United", "Kingdom"]
}
</script>

Each token is *embedded* <a class=footstart key='embedding'></a> into a high-dimensional numeric vector to facilitate computation. We represent this operation with a trapezoid. The embedded vector is the first *hidden representation* in the model.

<script type="application/x-vis+json">
{
  "type": "transformer-walkthrough",
  "jsonDataUrl": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/uk_example.json",
  "storyName": "initial-embedding"
}
</script>

After embedding, LLMs are organized into layers of transformer blocks<a class='citestart' key='vaswani2017'></a>. Each layer produces an updated hidden representation based on the output of the preceding layer, which we visualize with a circle labeled `$\ell^i$`, where `$i$` stands for the layer index. The hidden representation produced by the first transformer layer is labeled `$\ell^0$`.

<script type="application/x-vis+json">
{
  "type": "transformer-walkthrough",
  "jsonDataUrl": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/uk_example.json",
  "storyName": "initial-token-first-layer"
}
</script>

This procedure continues through the final layer, gradually updating the hidden representations to incorporate context about the task described in the prompt.

<script type="application/x-vis+json">
{
  "type": "transformer-walkthrough",
  "jsonDataUrl": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/uk_example.json",
  "storyName": "initial-token-full-row"
}
</script>

Next, subsequent prompt tokens are fed into the network<a class=footstart key='sequential'></a>. Tokens are stored in an ordered sequence, and the hidden representation produced by a layer for a given token is influenced by the hidden representation for that token at the preceding layer as well as the hidden representations for all preceding tokens in the sequence. Networks that only look backwards like this are called *decoder-only* models and are our focus here.

<script type="application/x-vis+json">
{
  "type": "transformer-walkthrough",
  "jsonDataUrl": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/uk_example.json",
  "storyName": "second-token-full-row"
}
</script>

After the final prompt token is processed, the final layer's hidden representation is used to yield an output token based on the distribution of the model's training data. This output token, <span class="token">is</span>, becomes the next input to the network.

<script type="application/x-vis+json">
{
  "type": "transformer-walkthrough",
  "jsonDataUrl": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/uk_example.json",
  "storyName": "first-generated-token"
}
</script>

By feeding generated tokens back in as input, the model can generate phrases, sentences, and even long-form text.

<script type="application/x-vis+json">
{
  "type": "transformer-walkthrough",
  "jsonDataUrl": "https://storage.googleapis.com/uncertainty-over-space/explorables/patching/uk_example.json",
  "storyName": "remaining-generated-tokens"
}
</script>

For a more in-depth, visual exploration of transformers and the attention mechanism, see the related YouTube videos from 3Blue1Brown<a class='citestart' key='3b1b-attn 3b1b-transformers'></a>.

### Appendix B: Formal description of Patchscopes

Given a hidden representation obtained from an LLM inference pass, a Patchscope instance decodes specific information from it by *patching* it into a different inference pass (of the same or a different LLM) that encourages the translation of that specific information.

Formally, given an input sequence of  `$n$` tokens `$S = \langle s_1, ..., s_{n} \rangle$` and a model `$M$` with `$L$` layers, `$\bm{h}_{i}^{\ell}$` denotes the hidden representation obtained at layer `$\ell \in [1, \ldots, L]$` and position `$i \in [1, \ldots, n]$`, when running `$M$` on `$S$`. To inspect `$\bm{h}_{i}^{\ell}$`, we consider a separate inference pass of a model `$M^*$` with `$L^*$` layers<a class='footstart' key='cross-model'></a> on a target sequence `$T = \langle t_1, \ldots, t_{m} \rangle$` of `$m$` tokens. Specifically, we choose a hidden representation `$\bar{\bm{h}}_{i^*}^{\ell^*}$` at layer `$\ell^* \in [1, \ldots, L^*]$` and position `${i^*} \in [1, \ldots, m]$` in the execution of `$M^*$` on `$T$`. Moreover, we define a mapping function `$f({\bm{h}}; \bm{\theta}): \mathbb{R}^{d} \mapsto \mathbb{R}^{d^*}$` parameterized by `$\bm{\theta}$` that operates on hidden representations of `$M$`, where `$d$` and `$d^*$` denote the hidden dimension of representations in `$M$` and `$M^*$`, respectively. This function can be the identity function, a linear or affine function learned on task-specific pairs of representations, or even more complex functions that incorporate other sources of data<a class='footstart' key='transformations'></a>.

The *patching* operation refers to dynamically replacing the representation `$\bar{\bm{h}}_{i^*}^{\ell^*}$` during the inference of `$M^*$` on `$T$` with `$f(\bm{h}_{i}^{\ell})$`.
Namely, by applying `${\bar{\bm{h}}}_{i^*}^{\ell^*} \leftarrow f(\bm{h}_{i}^{\ell})$`, we intervene on the generation process and modify the computation after layer `$\ell^*$`.

Overall, a Patchscope intervention applied to a representation determined by `$(S,i,M,\ell)$`, 
is defined by a quintuplet `$(T,i^*,f, M^*,\ell^*)$` of an inspection prompt `$T$`, a target position `$i^*$` in this prompt, a mapping function `$f$`, a target model `$M^*$`, and a target layer `$\ell^*$` of this model. It is possible that `$M$` and `$M^*$` are the same model, `$S$` and `$T$` are the same prompt, and `$f$` is the identity function `$\mathbb{I}$` (i.e., `$\mathbb{I}(\bm{h}) = \bm{h}$`).
### Footnotes
<a class='footend' key='author'></a>Equal contribution.

<a class='footend' key='author2'></a>Equal contribution.

<a class='footend' key='sae'></a>For example, [Anthropic](https://www.anthropic.com/news/mapping-mind-language-model) and [OpenAI](https://openai.com/index/extracting-concepts-from-gpt-4/) recently published work exploring how to disentangle activation vectors and think about using them to control model behavior, and Google's [Responsible Generative AI Toolkit](https://ai.google.dev/responsible) includes an interpretability-based prompt-debugger to help identify mistakes in the prompts fed to LLMs.

<a class='footend' key='prompt'></a> To score each generation, the following prompt was used with PaLM 2 Text Unicorn as the model:
<span class='code'>
<span class='fn-break'></span>
How semantically similar are the following texts: <text_A> {description} </text_A> <text_B> {generation} </text_B>
<span class='fn-break'></span>
Both these texts try to explain the following entity: &lt;entity&gt; {entity} &lt;/entity&gt;
<span class='fn-break'></span>
Provide an integer rating between 1 and 10.
<span class='fn-break'></span>
1 refers to 'not similar at all', and 10 refers to 'extremely similar': &lt;label&gt;
</span>

<a class='footend' key='dataset'></a> The full datasets are available on [Github](https://github.com/PAIR-code/interpretability/tree/master/patchscopes/code/preprocessed_data). While some of these examples are contrived to ease research evaluations, they do demonstrate the principle that rerouting hidden representations allows the model to generate more accurate conclusions.

<a class='footend' key='incorrect'></a>Note that the model can sometimes output incorrect information about the entity it is describing.

<a class='footend' key='probing-viz'></a> To get a more robust estimate of accuracy for patching, we average across five different source prompts for each example. A patching instance was considered correct if the correct answer appeared in any of the first 20 generated outputs of any target layer.

<a class='footend' key='probing-performance'></a> As we saw in the [prior case study](#case-study-verbalizing-the-model-s-thought-process), information about the input is more readily accessible in early to mid layers, before the model shifts toward next-token prediction. This explains why the performance of this simple Patchscope can drop for later source layers. In the [open questions section](#discussion-and-open-questions), we will discuss some strategies to improve attribute extraction accuracy in later layers by adjusting the Patchscope configurations.

<a class='footend' key='cot'></a> We want to caution that our primary goal in this section is not to devise a new method to solve multi-hop queries that is necessarily a competitor to CoT, but rather to make a proof-of-concept while comparing with CoT as a common reference.

<a class='footend' key='backwardpatch'></a>A *backward-acting* Patchscope extracts the hidden representation from one token and injects it into a preceding token in the prompt, thus this is only possible *when the source and inspection prompts are the same*. While the exact implications of patch directionality are unknown, we believe that this technique will be useful for understanding the information flow inside the model and may aid in identifying reasoning [circuits](https://transformer-circuits.pub/2021/framework/index.html), for example.

<a class='footend' key='widget-prompt'></a> To generate the entity descriptions, we use the same inspection prompt as seen in the entity description case study:
<span class='fn-break'></span>
<span class='code'>
Syria: Country in the Middle East, Leonardo DiCaprio: American actor, Samsung: South Korean multinational major appliance and consumer electronics corporation, x
</span>
<span class='fn-break'></span>
For each multi-hop query, we patch the hidden representation at the last token location of the selected source layer in place of the 'x' token of the inspection prompt at the last target layer. This generates the entity description of the hidden representation – here we are looking for some description of the correct country of origin as a checkpoint to determine whether the model will be able to answer the multi-hop query.

<a class='footend' key='placeholder-contamination'></a> Sometimes the remaining representation from the placeholder token in the early layers interferes with future token generations. We call this phenomenon *placeholder contamination*. It is only relevant if one is interested in generating more than the next immediate token, and it is more likely to happen if target location is in the later layers.

<a class='footend' key='circuits'></a> Mechanistic interpretability work that focuses on [circuits](https://transformer-circuits.pub/2021/framework/index.html) is one way of understanding information flow in a model. Prior work related to [indirect object identification](https://arxiv.org/pdf/2211.00593) and [factual associations](https://arxiv.org/pdf/2304.14767) are some concrete examples of how to provide precise localization guidelines for patching configurations. Conversely, we expect that investigations that use Patchscopes may result in the identification of new circuits in LLMs.

<a class='footend' key='tokens'></a>Tokenization strategies are complex and highly variable. Models are trained to work with specific tokenizers, and tokenizations may be incompatible across models.

<a class='footend' key='embedding'></a>The models we explore in this post use a deterministic embedding process to transform the token vector into the high-dimensional space of hidden representations. Most recent LLMs—including Gemma, Llama, and Mistral—use a single embedding layer for this task.

<a class='footend' key='sequential'></a>Technically speaking, transformer models don't just operate on prompt tokens one at a time. Rather, they process prompt tokens in parallel for efficiency. We visualize them here as entering sequentially to reinforce the idea that in decoder-only models, the processing of later tokens can only be influenced by earlier tokens.

<a class='footend' key='cross-model'></a> For simplicity, this post only covers case studies where the source and target model are the same. However, they *can* be different, and we briefly discuss in the [open questions section](#discussion-and-open-questions).

<a class='footend' key='transformations'></a> Patchscopes supports applying arbitrary transformations to the hidden representation from the source location  before it is injected into the target location. The examples in the post use the identity function as this transform (i.e., does nothing), but complex transforms may be necessary when patching across different models, or inspecting more complex functions.

### References
<a class='citeend' key='alain'></a>[Understanding intermediate layers using linear classifier probes](https://arxiv.org/abs/1610.01644) Alain, G. and Bengio, Y. , 5th International Conference on Learning Representations, Workshop Track Proceedings, 2017.

<a class='citeend' key='belinkov'></a> [Probing Classifiers: Promises, Shortcomings, and Advances](https://arxiv.org/abs/2102.12452) Belinkov, Y. Computational Linguistics, 48(1):207–219, 2022.

<a class='citeend' key='belrose'></a>[Eliciting Latent Predictions from Transformers with the Tuned Lens](https://arxiv.org/abs/2303.08112) Belrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky, I., McKinney, L., Biderman, S., and Steinhardt, J., arXiv preprint arXiv:2303.08112, 2023.

<a class='citeend' key='brown'></a> [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D., arXiv preprint arXiv:2005:14165, 2020.

<a class='citeend' key='selfie'></a>[SelfIE: Self-Interpretation of Large Language Model Embeddings.](https://arxiv.org/abs/2403.10949) Chen, H., Vondrick, C., & Mao, C.. arXiv preprint arXiv:2403.10949. 2024.

<a class='citeend' key='ghandeharioun'></a> [Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models](https://arxiv.org/pdf/2401.06102.pdf) Ghandeharioun, A.`$^*$`, Caciularu, A.`$^*$`, Pearce, A., Dixon, L., Geva, M. ICML (to appear), 2024.

<a class='citeend' key='whos-asking'></a>[Who’s asking? User personas and the mechanics of latent misalignment.](https://arxiv.org/abs/2406.12094) Ghandeharioun, A.*, Yuan, A.*, Guerard, M., Reif, E., Lepori, M.A., and Dixon, L., arXiv preprint arXiv:2406.12094, 2024.

<a class='citeend' key='tokenization'></a>[What do tokens know about their characters and how do they know it?](https://aclanthology.org/2022.naacl-main.179) Kaushal, A., Mahowald, K. 2022. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2487–2507, Seattle, United States. Association for Computational Linguistics.

<a class='citeend' key='meng'></a> [Locating and editing factual associations in GPT](https://arxiv.org/abs/2202.05262) Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Advances in Neural Information Processing Systems, 35:17359–17372, 2022.

<a class='citeend' key='llama'></a> [Introducing Meta Llama 3: The most capable openly available LLM to date.](https://ai.meta.com/blog/meta-llama-3/) Meta, 2023.

<a class='citeend' key='lesswrong'></a> [Interpreting GPT: the logit lens.](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/%20interpreting-gpt-the-logit-lens) nostalgebraist, LessWrong, 2020.

<a class='citeend' key='xstest'></a> [XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models.](https://arxiv.org/abs/2308.01263) Röttger, P., Kirk, H.R., Vidgen, B., Attanasio, G., Bianchi, F., and Hovy, D., arXiv preprint arXiv:2308.01263, 2023.

<a class='citeend' key='3b1b-attn'></a> [Attention in transformers, visually explained](https://www.youtube.com/watch?v=eMlx5fFNoYc&ab_channel=3Blue1Brown) Sanderson, G., 3Blue1Brown, 2024.

<a class='citeend' key='3b1b-transformers'></a> [But what is a GPT? Visual intro to transformers](https://www.youtube.com/watch?v=wjZofJX0v4M) Sanderson, G., 3Blue1Brown, 2024.

<a class='citeend' key='bert-rediscovers2019'></a> [BERT Rediscovers the Classical NLP Pipeline](https://aclanthology.org/P19-1452.pdf) Tenney, I., Das, D., Pavlick, E. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4593-4601. 2019.

<a class='citeend' key='vaswani2017'></a>[Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. Advances in neural information processing systems, 30. 2017.

<a class='citeend' key='bottomup2019'></a> [The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives](https://aclanthology.org/D19-1448.pdf) Voita, E., Sennrich, R., Titov, I. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019.

<a class='citeend' key='weiCoT'></a> [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V. and Zhou, D.. Advances in Neural Information Processing Systems 35, 2022.

<a class='citeend' key='autoeval'></a> [Judging llm-as-a-judge with mt-bench and chatbot arena.](https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html) Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. and Zhang, H. Advances in Neural Information Processing Systems 36, 2024.
### More Explorables

<p id='recirc'></p>
<div class='recirc-feedback-form'></div>


<link rel='stylesheet' href='../third_party/footnote_v2.css'>
<link rel='stylesheet' href='../third_party/citation_v2.css'>
<link rel='stylesheet' href='style.css'>

<script id='MathJax-script' async src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/mathtex-script-type.min.js" integrity="sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT" crossorigin="anonymous"></script>

<script src='../third_party/d3_.js'></script>
<script src='../third_party/d3-scale-chromatic.v1.min.js'></script>
<script src='../third_party/tfjsv3.18.0.js'></script>
<script src='../third_party/npyjs-global.js'></script>
<script src='../third_party/swoopy-drag.js'></script>

<script src='../third_party/footnote_v2.js'></script>
<script src='../third_party/citation_v2.js'></script>

<script defer src='../third_party/recirc.js'></script>

<script defer src="index.js" type="module"></script>